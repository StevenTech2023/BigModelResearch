{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/miniconda3/envs/falcon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /mnt/data1/miniconda3/envs/falcon/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /mnt/data1/miniconda3/envs/falcon/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/miniconda3/envs/falcon/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /mnt/data1/miniconda3/envs/falcon did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 9/9 [03:14<00:00, 21.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig ,BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/data1/shared/model/falcon-40b-instruct\", trust_remote_code=True, padding=False)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "#  /mnt/data1/shared/model/falcon-40b-instruct\n",
    "#  tiiuae/falcon-7b-instruct\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/mnt/data1/shared/model/falcon-40b-instruct\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=256,\n",
    "    top_p=1,\n",
    "    top_k=10,\n",
    "    repetition_penalty=1\n",
    ")\n",
    "generation_config.eos_token_id = generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 5 7 11 according to left number sequence, what is the next number and why?\n",
      "The next number in the sequence is 13, because it is the next prime number after 11. The sequence is based on the Fibonacci sequence, which is a sequence of numbers where each number is the sum of the two preceding numbers. In this case, the sequence is based on the leftmost digit of each number, so the sequence goes 2, 3, 5, 7, 11, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 128849, 2147483647, etc.\n"
     ]
    }
   ],
   "source": [
    "system_prompt ='''\n",
    "Suppose you are a math teacher\n",
    "'''\n",
    "\n",
    "input_text = '''\n",
    "2 3 5 7 11 according to left number sequence, what is the next number and why?\n",
    "'''\n",
    "\n",
    "input_ids = tokenizer.encode(system_prompt, input_text, return_tensors=\"pt\", padding=False, add_special_tokens=False).to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text = output_text[len(system_prompt):].strip()\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
